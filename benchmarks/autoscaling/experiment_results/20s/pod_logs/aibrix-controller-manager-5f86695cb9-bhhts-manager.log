W0124 22:17:51.647969       1 cache.go:154] environment variable AIBRIX_POD_METRIC_REFRESH_INTERVAL_MS is not set, using default value: 
I0124 22:17:51.648053       1 cache.go:139] using default refresh interval: 50 ms
I0124 22:17:51.652358       1 main.go:212] "msg"="starting cache" "logger"="setup"
I0124 22:17:51.652389       1 main.go:220] "msg"="using in-cluster configuration" "logger"="setup"
W0124 22:17:51.853320       1 cache.go:154] environment variable PROMETHEUS_ENDPOINT is not set, using default value: 
W0124 22:17:51.853368       1 cache.go:154] environment variable PROMETHEUS_BASIC_AUTH_USERNAME is not set, using default value: 
W0124 22:17:51.853375       1 cache.go:154] environment variable PROMETHEUS_BASIC_AUTH_PASSWORD is not set, using default value: 
I0124 22:17:51.853765       1 podautoscaler_controller.go:129] "Added AIBrix pod-autoscaler-controller successfully"
I0124 22:17:51.853801       1 podautoscaler_controller.go:133] "Run pod-autoscaler-controller periodical syncs successfully"
I0124 22:17:51.857964       1 modelrouter_controller.go:56] "Starting modelrouter controller"
I0124 22:17:51.861820       1 kvcache_controller.go:124] "Finished to add kv-cache-controller"
I0124 22:17:51.861878       1 main.go:257] "msg"="starting manager" "logger"="setup"
I0124 22:17:51.862341       1 server.go:50] "msg"="starting server" "addr"={"IP":"::","Port":8081,"Zone":""} "kind"="health probe"
I0124 22:17:52.022557       1 leaderelection.go:250] attempting to acquire leader lease aibrix-system/aibrix-controller-manager...
I0124 22:18:21.237207       1 leaderelection.go:260] successfully acquired lease aibrix-system/aibrix-controller-manager
I0124 22:18:21.237576       1 controller.go:178] "msg"="Starting EventSource" "controller"="rayclusterreplicaset" "controllerGroup"="orchestration.aibrix.ai" "controllerKind"="RayClusterReplicaSet" "source"="kind source: *v1alpha1.RayClusterReplicaSet"
I0124 22:18:21.237638       1 controller.go:178] "msg"="Starting EventSource" "controller"="kv-cache-controller" "controllerGroup"="orchestration.aibrix.ai" "controllerKind"="KVCache" "source"="kind source: *v1alpha1.KVCache"
I0124 22:18:21.237694       1 controller.go:178] "msg"="Starting EventSource" "controller"="kv-cache-controller" "controllerGroup"="orchestration.aibrix.ai" "controllerKind"="KVCache" "source"="kind source: *v1.Service"
I0124 22:18:21.237718       1 controller.go:178] "msg"="Starting EventSource" "controller"="kv-cache-controller" "controllerGroup"="orchestration.aibrix.ai" "controllerKind"="KVCache" "source"="kind source: *v1.Deployment"
I0124 22:18:21.237732       1 controller.go:178] "msg"="Starting EventSource" "controller"="kv-cache-controller" "controllerGroup"="orchestration.aibrix.ai" "controllerKind"="KVCache" "source"="kind source: *v1.Pod"
I0124 22:18:21.237663       1 controller.go:178] "msg"="Starting EventSource" "controller"="rayclusterreplicaset" "controllerGroup"="orchestration.aibrix.ai" "controllerKind"="RayClusterReplicaSet" "source"="kind source: *v1.RayCluster"
I0124 22:18:21.237769       1 controller.go:186] "msg"="Starting Controller" "controller"="rayclusterreplicaset" "controllerGroup"="orchestration.aibrix.ai" "controllerKind"="RayClusterReplicaSet"
I0124 22:18:21.237776       1 controller.go:186] "msg"="Starting Controller" "controller"="kv-cache-controller" "controllerGroup"="orchestration.aibrix.ai" "controllerKind"="KVCache"
I0124 22:18:21.237770       1 controller.go:178] "msg"="Starting EventSource" "controller"="rayclusterfleet" "controllerGroup"="orchestration.aibrix.ai" "controllerKind"="RayClusterFleet" "source"="kind source: *v1alpha1.RayClusterFleet"
I0124 22:18:21.237613       1 controller.go:178] "msg"="Starting EventSource" "controller"="model-adapter-controller" "controllerGroup"="model.aibrix.ai" "controllerKind"="ModelAdapter" "source"="kind source: *v1alpha1.ModelAdapter"
I0124 22:18:21.237813       1 controller.go:178] "msg"="Starting EventSource" "controller"="rayclusterfleet" "controllerGroup"="orchestration.aibrix.ai" "controllerKind"="RayClusterFleet" "source"="kind source: *v1alpha1.RayClusterReplicaSet"
I0124 22:18:21.237850       1 controller.go:178] "msg"="Starting EventSource" "controller"="model-adapter-controller" "controllerGroup"="model.aibrix.ai" "controllerKind"="ModelAdapter" "source"="kind source: *v1.Service"
I0124 22:18:21.237850       1 controller.go:178] "msg"="Starting EventSource" "controller"="rayclusterfleet" "controllerGroup"="orchestration.aibrix.ai" "controllerKind"="RayClusterFleet" "source"="kind source: *v1.RayCluster"
I0124 22:18:21.237875       1 controller.go:178] "msg"="Starting EventSource" "controller"="model-adapter-controller" "controllerGroup"="model.aibrix.ai" "controllerKind"="ModelAdapter" "source"="kind source: *v1.EndpointSlice"
I0124 22:18:21.237896       1 controller.go:178] "msg"="Starting EventSource" "controller"="model-adapter-controller" "controllerGroup"="model.aibrix.ai" "controllerKind"="ModelAdapter" "source"="kind source: *v1.Pod"
I0124 22:18:21.237921       1 controller.go:186] "msg"="Starting Controller" "controller"="model-adapter-controller" "controllerGroup"="model.aibrix.ai" "controllerKind"="ModelAdapter"
I0124 22:18:21.237983       1 controller.go:178] "msg"="Starting EventSource" "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "source"="kind source: *v1alpha1.PodAutoscaler"
I0124 22:18:21.238033       1 controller.go:178] "msg"="Starting EventSource" "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "source"="kind source: *v2.HorizontalPodAutoscaler"
I0124 22:18:21.237983       1 controller.go:186] "msg"="Starting Controller" "controller"="rayclusterfleet" "controllerGroup"="orchestration.aibrix.ai" "controllerKind"="RayClusterFleet"
I0124 22:18:21.238086       1 controller.go:178] "msg"="Starting EventSource" "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "source"="channel source: 0xc0008f3980"
I0124 22:18:21.238112       1 controller.go:186] "msg"="Starting Controller" "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler"
I0124 22:18:21.342789       1 controller.go:220] "msg"="Starting workers" "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "worker count"=1
I0124 22:18:21.342789       1 controller.go:220] "msg"="Starting workers" "controller"="model-adapter-controller" "controllerGroup"="model.aibrix.ai" "controllerKind"="ModelAdapter" "worker count"=1
I0124 22:18:21.344979       1 controller.go:220] "msg"="Starting workers" "controller"="rayclusterreplicaset" "controllerGroup"="orchestration.aibrix.ai" "controllerKind"="RayClusterReplicaSet" "worker count"=1
I0124 22:18:21.345000       1 controller.go:220] "msg"="Starting workers" "controller"="rayclusterfleet" "controllerGroup"="orchestration.aibrix.ai" "controllerKind"="RayClusterFleet" "worker count"=1
I0124 22:18:21.345037       1 controller.go:220] "msg"="Starting workers" "controller"="kv-cache-controller" "controllerGroup"="orchestration.aibrix.ai" "controllerKind"="KVCache" "worker count"=1
I0124 22:18:21.348242       1 podautoscaler_controller.go:671] "Scaler not found, creating new scaler" metricKey="default/aibrix-model-deepseek-llm-7b-chat" type="KPA"
I0124 22:18:21.348446       1 podautoscaler_controller.go:689] "New scaler added to AutoscalerMap" metricKey="default/aibrix-model-deepseek-llm-7b-chat" type="KPA" spec={"scaleTargetRef":{"kind":"Deployment","name":"aibrix-model-deepseek-llm-7b-chat","apiVersion":"apps/v1"},"minReplicas":1,"maxReplicas":10,"metricsSources":[{"metricSourceType":"pod","protocolType":"http","path":"metrics","port":"8000","targetMetric":"gpu_cache_usage_perc","targetValue":"0.5"}],"scalingStrategy":"KPA"}
E0124 22:18:21.350006       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.26:8000/metrics: Get \"http://10.0.1.26:8000/metrics\": dial tcp 10.0.1.26:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="0002d3b8-938a-4287-b9a1-8eab924f8ecf"
E0124 22:18:21.360425       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.26:8000/metrics: Get \"http://10.0.1.26:8000/metrics\": dial tcp 10.0.1.26:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="d55d0a2c-4fb6-4c69-bdf6-43a0142ac5b1"
E0124 22:18:21.377162       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.26:8000/metrics: Get \"http://10.0.1.26:8000/metrics\": dial tcp 10.0.1.26:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="eeab03e6-758f-4684-b647-c8ebe155201c"
E0124 22:18:21.402862       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.26:8000/metrics: Get \"http://10.0.1.26:8000/metrics\": dial tcp 10.0.1.26:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="392adc46-f0cf-4ab9-8d42-7c7e75426c27"
E0124 22:18:21.449232       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.26:8000/metrics: Get \"http://10.0.1.26:8000/metrics\": dial tcp 10.0.1.26:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="44645b28-aaf0-41a5-9177-b7bff9b636e6"
E0124 22:18:21.534972       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.26:8000/metrics: Get \"http://10.0.1.26:8000/metrics\": dial tcp 10.0.1.26:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="c4b2f3d2-d611-49e6-a6da-f8d8f52c57ea"
E0124 22:18:21.700683       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.26:8000/metrics: Get \"http://10.0.1.26:8000/metrics\": dial tcp 10.0.1.26:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="efdd899c-6722-43ff-9b2d-e6a13ba0a387"
E0124 22:18:21.859441       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.26:8000/metrics: Get \"http://10.0.1.26:8000/metrics\": dial tcp 10.0.1.26:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="98637fe2-3bc6-49f8-88c0-ff1a84f27428"
E0124 22:18:22.026698       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.26:8000/metrics: Get \"http://10.0.1.26:8000/metrics\": dial tcp 10.0.1.26:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="8c420c84-378e-4392-9c85-1fd4a04de6c0"
E0124 22:18:23.312431       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.26:8000/metrics: Get \"http://10.0.1.26:8000/metrics\": dial tcp 10.0.1.26:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="8cec1d82-ba35-454e-a6b0-0ba364b83f07"
E0124 22:18:25.878832       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.26:8000/metrics: Get \"http://10.0.1.26:8000/metrics\": dial tcp 10.0.1.26:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="ea30b802-cc68-4bac-ae06-ac03d3fb8c80"
E0124 22:18:31.004260       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.26:8000/metrics: Get \"http://10.0.1.26:8000/metrics\": dial tcp 10.0.1.26:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="0b85580d-1a63-44d1-a7a7-f1e10f6fc3fb"
E0124 22:18:31.859807       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.26:8000/metrics: Get \"http://10.0.1.26:8000/metrics\": dial tcp 10.0.1.26:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="be3829be-9e8b-4191-b842-6bcba19d964c"
E0124 22:18:41.249720       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.26:8000/metrics: Get \"http://10.0.1.26:8000/metrics\": dial tcp 10.0.1.26:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="2f5679e6-233a-438c-85ec-82c381867945"
E0124 22:18:41.860289       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.26:8000/metrics: Get \"http://10.0.1.26:8000/metrics\": dial tcp 10.0.1.26:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="2d36409a-0b36-4367-a733-31327bb8a63f"
E0124 22:18:51.865566       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to parse metrics from source http://10.0.1.26:8000/metrics: metrics gpu_cache_usage_perc not found" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="6e79498f-4e55-4621-a7c1-611729aeaa8c"
I0124 22:19:01.862040       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0
I0124 22:19:01.862108       1 client.go:105] "Update pod list metrics" metricKey="default/aibrix-model-deepseek-llm-7b-chat" valueNum=1 timestamp="2025-01-24 22:19:01.859695158 +0000 UTC m=+70.232059803" metricValue=0
I0124 22:19:01.862243       1 client.go:119] "Get panicWindow" metricKey="default/aibrix-model-deepseek-llm-7b-chat" panicValue=0 panicWindow="TimeWindow(granularity=1s, window=window(size=1, values=[{1737757141, 0.00}]))"
I0124 22:19:01.862262       1 client.go:126] Get stableWindow: metricKey=default/aibrix-model-deepseek-llm-7b-chat, stableValue=0.00, stableWindow=TimeWindow(granularity=1s, window=window(size=1, values=[{1737757141, 0.00}]))
I0124 22:19:01.862289       1 kpa.go:343] "Operating in stable mode." desiredPodCount=0
I0124 22:19:01.862327       1 podautoscaler_controller.go:433] "Proposing desired replicas" desiredReplicas=0 metric="gpu_cache_usage_perc" timestamp="2025-01-24 22:19:01.862118537 +0000 UTC m=+70.234483173" scaleTarget="Deployment/default/aibrix-model-deepseek-llm-7b-chat"
I0124 22:19:01.862346       1 podautoscaler_controller.go:457] "Scaling adjustment: Algorithm recommended scaling to a target that fell below the minimum limit." recommendedReplicas=0 adjustedTo=1
I0124 22:19:11.861017       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0
I0124 22:19:11.861070       1 client.go:105] "Update pod list metrics" metricKey="default/aibrix-model-deepseek-llm-7b-chat" valueNum=1 timestamp="2025-01-24 22:19:11.858721619 +0000 UTC m=+80.231086274" metricValue=0
I0124 22:19:11.861196       1 client.go:119] "Get panicWindow" metricKey="default/aibrix-model-deepseek-llm-7b-chat" panicValue=0 panicWindow="TimeWindow(granularity=1s, window=window(size=1, values=[{1737757151, 0.00}]))"
I0124 22:19:11.861210       1 client.go:126] Get stableWindow: metricKey=default/aibrix-model-deepseek-llm-7b-chat, stableValue=0.00, stableWindow=TimeWindow(granularity=1s, window=window(size=2, values=[{1737757141, 0.00}, {1737757151, 0.00}]))
I0124 22:19:11.861229       1 kpa.go:343] "Operating in stable mode." desiredPodCount=0
I0124 22:19:11.861249       1 podautoscaler_controller.go:433] "Proposing desired replicas" desiredReplicas=0 metric="gpu_cache_usage_perc" timestamp="2025-01-24 22:19:11.861078839 +0000 UTC m=+80.233443470" scaleTarget="Deployment/default/aibrix-model-deepseek-llm-7b-chat"
I0124 22:19:11.861262       1 podautoscaler_controller.go:457] "Scaling adjustment: Algorithm recommended scaling to a target that fell below the minimum limit." recommendedReplicas=0 adjustedTo=1
I0124 22:19:21.862831       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0
I0124 22:19:21.862890       1 client.go:105] "Update pod list metrics" metricKey="default/aibrix-model-deepseek-llm-7b-chat" valueNum=1 timestamp="2025-01-24 22:19:21.859627847 +0000 UTC m=+90.231992511" metricValue=0
I0124 22:19:21.863042       1 client.go:119] "Get panicWindow" metricKey="default/aibrix-model-deepseek-llm-7b-chat" panicValue=0 panicWindow="TimeWindow(granularity=1s, window=window(size=1, values=[{1737757161, 0.00}]))"
I0124 22:19:21.863074       1 client.go:126] Get stableWindow: metricKey=default/aibrix-model-deepseek-llm-7b-chat, stableValue=0.00, stableWindow=TimeWindow(granularity=1s, window=window(size=3, values=[{1737757141, 0.00}, {1737757151, 0.00}, {1737757161, 0.00}]))
I0124 22:19:21.863105       1 kpa.go:343] "Operating in stable mode." desiredPodCount=0
I0124 22:19:21.863242       1 podautoscaler_controller.go:433] "Proposing desired replicas" desiredReplicas=0 metric="gpu_cache_usage_perc" timestamp="2025-01-24 22:19:21.862897799 +0000 UTC m=+90.235262427" scaleTarget="Deployment/default/aibrix-model-deepseek-llm-7b-chat"
I0124 22:19:21.863403       1 podautoscaler_controller.go:457] "Scaling adjustment: Algorithm recommended scaling to a target that fell below the minimum limit." recommendedReplicas=0 adjustedTo=1
I0124 22:19:22.216968       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0
I0124 22:19:22.217015       1 client.go:105] "Update pod list metrics" metricKey="default/aibrix-model-deepseek-llm-7b-chat" valueNum=1 timestamp="2025-01-24 22:19:22.214989291 +0000 UTC m=+90.587353943" metricValue=0
I0124 22:19:22.217158       1 client.go:119] "Get panicWindow" metricKey="default/aibrix-model-deepseek-llm-7b-chat" panicValue=0 panicWindow="TimeWindow(granularity=1s, window=window(size=2, values=[{1737757161, 0.00}, {1737757162, 0.00}]))"
I0124 22:19:22.217175       1 client.go:126] Get stableWindow: metricKey=default/aibrix-model-deepseek-llm-7b-chat, stableValue=0.00, stableWindow=TimeWindow(granularity=1s, window=window(size=4, values=[{1737757141, 0.00}, {1737757151, 0.00}, {1737757161, 0.00}, {1737757162, 0.00}]))
I0124 22:19:22.217206       1 kpa.go:343] "Operating in stable mode." desiredPodCount=0
I0124 22:19:22.217236       1 podautoscaler_controller.go:433] "Proposing desired replicas" desiredReplicas=0 metric="gpu_cache_usage_perc" timestamp="2025-01-24 22:19:22.217023411 +0000 UTC m=+90.589388037" scaleTarget="Deployment/default/aibrix-model-deepseek-llm-7b-chat"
I0124 22:19:22.217248       1 podautoscaler_controller.go:457] "Scaling adjustment: Algorithm recommended scaling to a target that fell below the minimum limit." recommendedReplicas=0 adjustedTo=1
I0124 22:19:31.861468       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0
I0124 22:19:31.861515       1 client.go:105] "Update pod list metrics" metricKey="default/aibrix-model-deepseek-llm-7b-chat" valueNum=1 timestamp="2025-01-24 22:19:31.859301979 +0000 UTC m=+100.231666621" metricValue=0
I0124 22:19:31.861625       1 client.go:119] "Get panicWindow" metricKey="default/aibrix-model-deepseek-llm-7b-chat" panicValue=0 panicWindow="TimeWindow(granularity=1s, window=window(size=2, values=[{1737757162, 0.00}, {1737757171, 0.00}]))"
I0124 22:19:31.861637       1 client.go:126] Get stableWindow: metricKey=default/aibrix-model-deepseek-llm-7b-chat, stableValue=0.00, stableWindow=TimeWindow(granularity=1s, window=window(size=5, values=[{1737757141, 0.00}, {1737757151, 0.00}, {1737757161, 0.00}, {1737757162, 0.00}, {1737757171, 0.00}]))
I0124 22:19:31.861660       1 kpa.go:343] "Operating in stable mode." desiredPodCount=0
I0124 22:19:31.861680       1 podautoscaler_controller.go:433] "Proposing desired replicas" desiredReplicas=0 metric="gpu_cache_usage_perc" timestamp="2025-01-24 22:19:31.861521468 +0000 UTC m=+100.233886097" scaleTarget="Deployment/default/aibrix-model-deepseek-llm-7b-chat"
I0124 22:19:31.861688       1 podautoscaler_controller.go:457] "Scaling adjustment: Algorithm recommended scaling to a target that fell below the minimum limit." recommendedReplicas=0 adjustedTo=1
I0124 22:19:41.870911       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.11111111111111116
I0124 22:19:41.870961       1 client.go:105] "Update pod list metrics" metricKey="default/aibrix-model-deepseek-llm-7b-chat" valueNum=1 timestamp="2025-01-24 22:19:41.859043425 +0000 UTC m=+110.231408112" metricValue=0.11111111111111116
I0124 22:19:41.871105       1 client.go:119] "Get panicWindow" metricKey="default/aibrix-model-deepseek-llm-7b-chat" panicValue=0.11111111111111116 panicWindow="TimeWindow(granularity=1s, window=window(size=1, values=[{1737757181, 0.11}]))"
I0124 22:19:41.871121       1 client.go:126] Get stableWindow: metricKey=default/aibrix-model-deepseek-llm-7b-chat, stableValue=0.02, stableWindow=TimeWindow(granularity=1s, window=window(size=6, values=[{1737757141, 0.00}, {1737757151, 0.00}, {1737757161, 0.00}, {1737757162, 0.00}, {1737757171, 0.00}, {1737757181, 0.11}]))
I0124 22:19:41.871152       1 kpa.go:343] "Operating in stable mode." desiredPodCount=1
I0124 22:19:41.871187       1 podautoscaler_controller.go:433] "Proposing desired replicas" desiredReplicas=1 metric="gpu_cache_usage_perc" timestamp="2025-01-24 22:19:41.870968726 +0000 UTC m=+110.243333355" scaleTarget="Deployment/default/aibrix-model-deepseek-llm-7b-chat"
I0124 22:19:51.862215       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9193302891933028
I0124 22:19:51.862257       1 client.go:105] "Update pod list metrics" metricKey="default/aibrix-model-deepseek-llm-7b-chat" valueNum=1 timestamp="2025-01-24 22:19:51.85898749 +0000 UTC m=+120.231352146" metricValue=0.9193302891933028
I0124 22:19:51.862399       1 client.go:119] "Get panicWindow" metricKey="default/aibrix-model-deepseek-llm-7b-chat" panicValue=0.9193302891933028 panicWindow="TimeWindow(granularity=1s, window=window(size=1, values=[{1737757191, 0.92}]))"
I0124 22:19:51.862417       1 client.go:126] Get stableWindow: metricKey=default/aibrix-model-deepseek-llm-7b-chat, stableValue=0.15, stableWindow=TimeWindow(granularity=1s, window=window(size=7, values=[{1737757141, 0.00}, {1737757151, 0.00}, {1737757161, 0.00}, {1737757162, 0.00}, {1737757171, 0.00}, {1737757181, 0.11}, {1737757191, 0.92}]))
I0124 22:19:51.862444       1 kpa.go:309] "Begin panicking." panicTime="2025-01-24 22:19:51.862263349 +0000 UTC m=+120.234627975"
I0124 22:19:51.862461       1 kpa.go:330] "Operating in panic mode." desiredPodCount=1 desiredPanicPodCount=2
I0124 22:19:51.862479       1 kpa.go:336] "Increasing pods count." originalPodCount=1 desiredPodCount=2
I0124 22:19:51.862518       1 podautoscaler_controller.go:433] "Proposing desired replicas" desiredReplicas=2 metric="gpu_cache_usage_perc" timestamp="2025-01-24 22:19:51.862263349 +0000 UTC m=+120.234627975" scaleTarget="Deployment/default/aibrix-model-deepseek-llm-7b-chat"
I0124 22:19:51.876577       1 podautoscaler_controller.go:488] "Successfully rescaled" PodAutoscaler="default/podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" currentReplicas=1 desiredReplicas=2 reason="gpu_cache_usage_perc above target"
I0124 22:20:01.864334       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9056316590563166
E0124 22:20:01.865357       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.69:8000/metrics: Get \"http://10.0.0.69:8000/metrics\": dial tcp 10.0.0.69:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="e974d801-8eed-4670-a3bf-a069b0ceb92f"
I0124 22:20:01.879056       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9056316590563166
E0124 22:20:01.879853       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.69:8000/metrics: Get \"http://10.0.0.69:8000/metrics\": dial tcp 10.0.0.69:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="132656ca-4cef-48ee-95b2-dc8b43c559fa"
I0124 22:20:01.898421       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9056316590563166
E0124 22:20:01.899227       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.69:8000/metrics: Get \"http://10.0.0.69:8000/metrics\": dial tcp 10.0.0.69:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="11e705c7-408f-4dd7-87e5-47415f4d48db"
E0124 22:20:01.925209       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.69:8000/metrics: Get \"http://10.0.0.69:8000/metrics\": dial tcp 10.0.0.69:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="408fcbf8-1922-497b-8921-f5fc16703b1e"
I0124 22:20:01.972915       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9690512430238458
E0124 22:20:01.973733       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.69:8000/metrics: Get \"http://10.0.0.69:8000/metrics\": dial tcp 10.0.0.69:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="8d4121e0-dfba-47f8-94b5-e83cab000c07"
I0124 22:20:02.062700       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9690512430238458
E0124 22:20:02.063422       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.69:8000/metrics: Get \"http://10.0.0.69:8000/metrics\": dial tcp 10.0.0.69:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="2828d56e-f292-4e5a-a879-9c4ad29a1c83"
I0124 22:20:02.234623       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9690512430238458
E0124 22:20:02.235471       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.69:8000/metrics: Get \"http://10.0.0.69:8000/metrics\": dial tcp 10.0.0.69:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="e944d2e4-8905-4f01-a5ca-489b9e20922d"
I0124 22:20:02.564046       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9654997463216641
E0124 22:20:02.564932       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.69:8000/metrics: Get \"http://10.0.0.69:8000/metrics\": dial tcp 10.0.0.69:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="31d307f2-d612-48d9-9b26-808fea18dbbb"
I0124 22:20:03.214151       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9802130898021308
E0124 22:20:03.215011       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.69:8000/metrics: Get \"http://10.0.0.69:8000/metrics\": dial tcp 10.0.0.69:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="3df738e3-8502-429f-8c11-079c2b94c2fa"
I0124 22:20:04.503217       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9786910197869102
E0124 22:20:04.504134       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.69:8000/metrics: Get \"http://10.0.0.69:8000/metrics\": dial tcp 10.0.0.69:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="1d78789d-a49f-4247-8cee-a41a2d0df3ae"
I0124 22:20:07.072829       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9355657026889903
E0124 22:20:07.073690       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.69:8000/metrics: Get \"http://10.0.0.69:8000/metrics\": dial tcp 10.0.0.69:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="0d802b4d-23d6-41cf-96b3-516f2ab455a1"
I0124 22:20:11.862273       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.884830035514967
E0124 22:20:11.863318       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.69:8000/metrics: Get \"http://10.0.0.69:8000/metrics\": dial tcp 10.0.0.69:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="e946ad00-642a-4694-a8c1-af833afc2a6d"
I0124 22:20:12.201085       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9472349061390157
E0124 22:20:12.201953       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.69:8000/metrics: Get \"http://10.0.0.69:8000/metrics\": dial tcp 10.0.0.69:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="00243723-6d91-46df-b03e-2b9b733655d8"
I0124 22:20:21.864031       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9812278031456114
E0124 22:20:21.864872       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.69:8000/metrics: Get \"http://10.0.0.69:8000/metrics\": dial tcp 10.0.0.69:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="663e879d-2f0f-4318-95f3-558a2e4f6db0"
I0124 22:20:31.863245       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9949264332825977
E0124 22:20:31.865405       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.69:8000/metrics: Get \"http://10.0.0.69:8000/metrics\": dial tcp 10.0.0.69:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="3f66ece0-71c3-4ced-9ea7-787c71c497ce"
I0124 22:20:32.689469       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9665144596651446
E0124 22:20:32.690409       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.69:8000/metrics: Get \"http://10.0.0.69:8000/metrics\": dial tcp 10.0.0.69:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="25113aa5-37a5-4eed-849f-5f0a94c994a1"
I0124 22:20:41.863349       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.976154236428209
E0124 22:20:41.866355       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to parse metrics from source http://10.0.0.69:8000/metrics: metrics gpu_cache_usage_perc not found" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="a2aa80cd-6a14-403f-ab5e-3a22ef02e541"
I0124 22:20:51.862781       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9548452562151193
I0124 22:20:51.865764       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0
I0124 22:20:51.865812       1 client.go:105] "Update pod list metrics" metricKey="default/aibrix-model-deepseek-llm-7b-chat" valueNum=2 timestamp="2025-01-24 22:20:51.858837278 +0000 UTC m=+180.231201910" metricValue=0.9548452562151193
I0124 22:20:51.865945       1 client.go:119] "Get panicWindow" metricKey="default/aibrix-model-deepseek-llm-7b-chat" panicValue=0.9548452562151193 panicWindow="TimeWindow(granularity=1s, window=window(size=1, values=[{1737757251, 0.95}]))"
I0124 22:20:51.865960       1 client.go:126] Get stableWindow: metricKey=default/aibrix-model-deepseek-llm-7b-chat, stableValue=0.95, stableWindow=TimeWindow(granularity=1s, window=window(size=1, values=[{1737757251, 0.95}]))
I0124 22:20:51.865976       1 kpa.go:330] "Operating in panic mode." desiredPodCount=2 desiredPanicPodCount=2
I0124 22:20:51.865996       1 podautoscaler_controller.go:433] "Proposing desired replicas" desiredReplicas=2 metric="gpu_cache_usage_perc" timestamp="2025-01-24 22:20:51.865819153 +0000 UTC m=+180.238183777" scaleTarget="Deployment/default/aibrix-model-deepseek-llm-7b-chat"
I0124 22:21:01.863120       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9766615930999493
I0124 22:21:01.866285       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0
I0124 22:21:01.866322       1 client.go:105] "Update pod list metrics" metricKey="default/aibrix-model-deepseek-llm-7b-chat" valueNum=2 timestamp="2025-01-24 22:21:01.859448833 +0000 UTC m=+190.231813488" metricValue=0.9766615930999493
I0124 22:21:01.866942       1 client.go:119] "Get panicWindow" metricKey="default/aibrix-model-deepseek-llm-7b-chat" panicValue=0.9766615930999493 panicWindow="TimeWindow(granularity=1s, window=window(size=1, values=[{1737757261, 0.98}]))"
I0124 22:21:01.866971       1 client.go:126] Get stableWindow: metricKey=default/aibrix-model-deepseek-llm-7b-chat, stableValue=0.97, stableWindow=TimeWindow(granularity=1s, window=window(size=2, values=[{1737757251, 0.95}, {1737757261, 0.98}]))
I0124 22:21:01.867008       1 kpa.go:330] "Operating in panic mode." desiredPodCount=2 desiredPanicPodCount=2
I0124 22:21:01.867045       1 podautoscaler_controller.go:433] "Proposing desired replicas" desiredReplicas=2 metric="gpu_cache_usage_perc" timestamp="2025-01-24 22:21:01.86632959 +0000 UTC m=+190.238694216" scaleTarget="Deployment/default/aibrix-model-deepseek-llm-7b-chat"
I0124 22:21:11.863974       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9619482496194824
I0124 22:21:11.867604       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0
I0124 22:21:11.867646       1 client.go:105] "Update pod list metrics" metricKey="default/aibrix-model-deepseek-llm-7b-chat" valueNum=2 timestamp="2025-01-24 22:21:11.859693457 +0000 UTC m=+200.232058091" metricValue=0.9619482496194824
I0124 22:21:11.867822       1 client.go:119] "Get panicWindow" metricKey="default/aibrix-model-deepseek-llm-7b-chat" panicValue=0.9619482496194824 panicWindow="TimeWindow(granularity=1s, window=window(size=1, values=[{1737757271, 0.96}]))"
I0124 22:21:11.867841       1 client.go:126] Get stableWindow: metricKey=default/aibrix-model-deepseek-llm-7b-chat, stableValue=0.96, stableWindow=TimeWindow(granularity=1s, window=window(size=3, values=[{1737757251, 0.95}, {1737757261, 0.98}, {1737757271, 0.96}]))
I0124 22:21:11.867868       1 kpa.go:330] "Operating in panic mode." desiredPodCount=2 desiredPanicPodCount=2
I0124 22:21:11.867903       1 podautoscaler_controller.go:433] "Proposing desired replicas" desiredReplicas=2 metric="gpu_cache_usage_perc" timestamp="2025-01-24 22:21:11.867655257 +0000 UTC m=+200.240019885" scaleTarget="Deployment/default/aibrix-model-deepseek-llm-7b-chat"
I0124 22:21:21.862073       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9553526128868595
I0124 22:21:21.865034       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0
I0124 22:21:21.865088       1 client.go:105] "Update pod list metrics" metricKey="default/aibrix-model-deepseek-llm-7b-chat" valueNum=2 timestamp="2025-01-24 22:21:21.858936049 +0000 UTC m=+210.231300679" metricValue=0.9553526128868595
I0124 22:21:21.865255       1 client.go:119] "Get panicWindow" metricKey="default/aibrix-model-deepseek-llm-7b-chat" panicValue=0.9553526128868595 panicWindow="TimeWindow(granularity=1s, window=window(size=1, values=[{1737757281, 0.96}]))"
I0124 22:21:21.865275       1 client.go:126] Get stableWindow: metricKey=default/aibrix-model-deepseek-llm-7b-chat, stableValue=0.96, stableWindow=TimeWindow(granularity=1s, window=window(size=4, values=[{1737757251, 0.95}, {1737757261, 0.98}, {1737757271, 0.96}, {1737757281, 0.96}]))
I0124 22:21:21.865295       1 kpa.go:330] "Operating in panic mode." desiredPodCount=2 desiredPanicPodCount=2
I0124 22:21:21.865321       1 podautoscaler_controller.go:433] "Proposing desired replicas" desiredReplicas=2 metric="gpu_cache_usage_perc" timestamp="2025-01-24 22:21:21.865096971 +0000 UTC m=+210.237461595" scaleTarget="Deployment/default/aibrix-model-deepseek-llm-7b-chat"
I0124 22:21:31.864248       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9299847792998478
I0124 22:21:31.866832       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0
I0124 22:21:31.866865       1 client.go:105] "Update pod list metrics" metricKey="default/aibrix-model-deepseek-llm-7b-chat" valueNum=2 timestamp="2025-01-24 22:21:31.859232635 +0000 UTC m=+220.231597312" metricValue=0.9299847792998478
I0124 22:21:31.866984       1 client.go:119] "Get panicWindow" metricKey="default/aibrix-model-deepseek-llm-7b-chat" panicValue=0.9299847792998478 panicWindow="TimeWindow(granularity=1s, window=window(size=1, values=[{1737757291, 0.93}]))"
I0124 22:21:31.867000       1 client.go:126] Get stableWindow: metricKey=default/aibrix-model-deepseek-llm-7b-chat, stableValue=0.96, stableWindow=TimeWindow(granularity=1s, window=window(size=5, values=[{1737757251, 0.95}, {1737757261, 0.98}, {1737757271, 0.96}, {1737757281, 0.96}, {1737757291, 0.93}]))
I0124 22:21:31.867030       1 kpa.go:330] "Operating in panic mode." desiredPodCount=2 desiredPanicPodCount=2
I0124 22:21:31.867085       1 podautoscaler_controller.go:433] "Proposing desired replicas" desiredReplicas=2 metric="gpu_cache_usage_perc" timestamp="2025-01-24 22:21:31.866871762 +0000 UTC m=+220.239236386" scaleTarget="Deployment/default/aibrix-model-deepseek-llm-7b-chat"
I0124 22:21:41.863219       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.984779299847793
I0124 22:21:41.866772       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.26382546930492135
I0124 22:21:41.866813       1 client.go:105] "Update pod list metrics" metricKey="default/aibrix-model-deepseek-llm-7b-chat" valueNum=2 timestamp="2025-01-24 22:21:41.860056897 +0000 UTC m=+230.232421566" metricValue=1.2486047691527142
I0124 22:21:41.866968       1 client.go:119] "Get panicWindow" metricKey="default/aibrix-model-deepseek-llm-7b-chat" panicValue=1.2486047691527142 panicWindow="TimeWindow(granularity=1s, window=window(size=1, values=[{1737757301, 1.25}]))"
I0124 22:21:41.866983       1 client.go:126] Get stableWindow: metricKey=default/aibrix-model-deepseek-llm-7b-chat, stableValue=1.00, stableWindow=TimeWindow(granularity=1s, window=window(size=6, values=[{1737757251, 0.95}, {1737757261, 0.98}, {1737757271, 0.96}, {1737757281, 0.96}, {1737757291, 0.93}, {1737757301, 1.25}]))
I0124 22:21:41.867016       1 kpa.go:330] "Operating in panic mode." desiredPodCount=3 desiredPanicPodCount=3
I0124 22:21:41.867038       1 kpa.go:336] "Increasing pods count." originalPodCount=2 desiredPodCount=3
I0124 22:21:41.867102       1 podautoscaler_controller.go:433] "Proposing desired replicas" desiredReplicas=3 metric="gpu_cache_usage_perc" timestamp="2025-01-24 22:21:41.866822513 +0000 UTC m=+230.239187143" scaleTarget="Deployment/default/aibrix-model-deepseek-llm-7b-chat"
I0124 22:21:41.880332       1 podautoscaler_controller.go:488] "Successfully rescaled" PodAutoscaler="default/podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" currentReplicas=2 desiredReplicas=3 reason="gpu_cache_usage_perc above target"
I0124 22:21:51.863469       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.8335870116692035
I0124 22:21:51.873801       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9898528665651953
E0124 22:21:51.874426       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.28:8000/metrics: Get \"http://10.0.1.28:8000/metrics\": dial tcp 10.0.1.28:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="222caa46-565f-4f1d-b786-01f9f37a822f"
I0124 22:21:51.886143       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.8335870116692035
I0124 22:21:51.895232       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9898528665651953
E0124 22:21:51.895650       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.28:8000/metrics: Get \"http://10.0.1.28:8000/metrics\": dial tcp 10.0.1.28:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="804712f3-a4af-4743-a2f2-c868439bce16"
I0124 22:21:51.912903       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9898528665651953
E0124 22:21:51.913406       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.28:8000/metrics: Get \"http://10.0.1.28:8000/metrics\": dial tcp 10.0.1.28:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="fec4ab0b-7b4b-4566-9e1e-e756c1b6a1c3"
I0124 22:21:51.940579       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.8340943683409436
I0124 22:21:51.957981       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9898528665651953
E0124 22:21:51.958356       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.28:8000/metrics: Get \"http://10.0.1.28:8000/metrics\": dial tcp 10.0.1.28:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="3a780d86-7c2f-488a-a2dd-7bd55b335d4e"
I0124 22:21:52.006148       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9908675799086758
E0124 22:21:52.006563       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.28:8000/metrics: Get \"http://10.0.1.28:8000/metrics\": dial tcp 10.0.1.28:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="47eb4a09-203b-452a-bd7c-e5f84a4811ee"
I0124 22:21:52.094567       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.8356164383561644
I0124 22:21:52.097465       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9928970065956367
E0124 22:21:52.097895       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.28:8000/metrics: Get \"http://10.0.1.28:8000/metrics\": dial tcp 10.0.1.28:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="2f3e5ffa-afd0-4145-a0af-afee925a763d"
I0124 22:21:52.264417       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.8386605783866058
I0124 22:21:52.271215       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9969558599695586
E0124 22:21:52.271624       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.28:8000/metrics: Get \"http://10.0.1.28:8000/metrics\": dial tcp 10.0.1.28:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="2d2c1e4d-8458-456a-a2ea-716f83c6649d"
I0124 22:21:52.599627       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.8452562151192289
I0124 22:21:52.602880       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9715880263825469
E0124 22:21:52.603323       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.28:8000/metrics: Get \"http://10.0.1.28:8000/metrics\": dial tcp 10.0.1.28:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="47cc6e25-c545-466e-8179-1a8c706863b7"
I0124 22:21:53.251389       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.7950279046169457
I0124 22:21:53.254571       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9167935058346017
E0124 22:21:53.255054       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.28:8000/metrics: Get \"http://10.0.1.28:8000/metrics\": dial tcp 10.0.1.28:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="b5c128ff-3652-4f3c-9470-2b296c102079"
I0124 22:21:54.543433       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.8564180618975139
I0124 22:21:54.546626       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9766615930999493
E0124 22:21:54.547098       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.28:8000/metrics: Get \"http://10.0.1.28:8000/metrics\": dial tcp 10.0.1.28:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="86887296-bcfe-43bb-86df-f364fc854467"
I0124 22:21:57.117027       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9837645865043125
I0124 22:21:57.120168       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9994926433282598
E0124 22:21:57.120616       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.28:8000/metrics: Get \"http://10.0.1.28:8000/metrics\": dial tcp 10.0.1.28:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="a0ebb2f1-47fd-41fe-9d1b-ebcea283a8ed"
I0124 22:22:01.864046       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9512937595129376
I0124 22:22:01.866929       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9893455098934552
E0124 22:22:01.867431       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.28:8000/metrics: Get \"http://10.0.1.28:8000/metrics\": dial tcp 10.0.1.28:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="909f5b24-2fc7-43e9-b66c-9798bc093ef1"
I0124 22:22:02.249363       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.943683409436834
I0124 22:22:02.252995       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9949264332825977
E0124 22:22:02.253431       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.28:8000/metrics: Get \"http://10.0.1.28:8000/metrics\": dial tcp 10.0.1.28:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="d1bff539-1523-4f48-b3d7-3b505cdeeaa0"
I0124 22:22:11.863963       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9842719431760527
I0124 22:22:11.868333       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9883307965499746
E0124 22:22:11.868781       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.28:8000/metrics: Get \"http://10.0.1.28:8000/metrics\": dial tcp 10.0.1.28:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="715edcf2-8a11-431a-aa52-8052973f915b"
I0124 22:22:21.862783       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9857940131912735
I0124 22:22:21.866348       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9964485032978183
E0124 22:22:21.866811       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.28:8000/metrics: Get \"http://10.0.1.28:8000/metrics\": dial tcp 10.0.1.28:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="59b763f1-d39b-490b-808d-1d52db0a5af6"
I0124 22:22:22.742616       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9979705733130391
I0124 22:22:22.745732       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9558599695585996
E0124 22:22:22.746219       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.1.28:8000/metrics: Get \"http://10.0.1.28:8000/metrics\": dial tcp 10.0.1.28:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="40f12973-1c36-4014-96f4-567f90f867d6"
I0124 22:22:31.862938       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9852866565195333
I0124 22:22:31.866663       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9563673262303399
E0124 22:22:31.873358       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to parse metrics from source http://10.0.1.28:8000/metrics: metrics gpu_cache_usage_perc not found" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="8d985c23-9bf9-4aca-8ee2-1a1dd111611a"
I0124 22:22:41.862018       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0
I0124 22:22:41.865046       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9690512430238458
I0124 22:22:41.868566       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9959411466260781
I0124 22:22:41.868601       1 client.go:105] "Update pod list metrics" metricKey="default/aibrix-model-deepseek-llm-7b-chat" valueNum=3 timestamp="2025-01-24 22:22:41.859487396 +0000 UTC m=+290.231852066" metricValue=1.964992389649924
I0124 22:22:41.868767       1 client.go:119] "Get panicWindow" metricKey="default/aibrix-model-deepseek-llm-7b-chat" panicValue=1.964992389649924 panicWindow="TimeWindow(granularity=1s, window=window(size=1, values=[{1737757361, 1.96}]))"
I0124 22:22:41.868784       1 client.go:126] Get stableWindow: metricKey=default/aibrix-model-deepseek-llm-7b-chat, stableValue=1.96, stableWindow=TimeWindow(granularity=1s, window=window(size=1, values=[{1737757361, 1.96}]))
I0124 22:22:41.868802       1 kpa.go:330] "Operating in panic mode." desiredPodCount=4 desiredPanicPodCount=4
I0124 22:22:41.868815       1 kpa.go:336] "Increasing pods count." originalPodCount=2 desiredPodCount=4
I0124 22:22:41.868852       1 podautoscaler_controller.go:433] "Proposing desired replicas" desiredReplicas=4 metric="gpu_cache_usage_perc" timestamp="2025-01-24 22:22:41.868607177 +0000 UTC m=+290.240971799" scaleTarget="Deployment/default/aibrix-model-deepseek-llm-7b-chat"
I0124 22:22:41.881975       1 podautoscaler_controller.go:488] "Successfully rescaled" PodAutoscaler="default/podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" currentReplicas=3 desiredReplicas=4 reason="gpu_cache_usage_perc above target"
I0124 22:22:51.863276       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9959411466260781
I0124 22:22:51.867443       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9959411466260781
I0124 22:22:51.869774       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0
E0124 22:22:51.870550       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.59:8000/metrics: Get \"http://10.0.0.59:8000/metrics\": dial tcp 10.0.0.59:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="cdd0b0c4-0bab-4a74-8038-ab215ae768e6"
I0124 22:22:51.882890       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9969558599695586
I0124 22:22:51.885752       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9959411466260781
I0124 22:22:51.887401       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0
E0124 22:22:51.888168       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.59:8000/metrics: Get \"http://10.0.0.59:8000/metrics\": dial tcp 10.0.0.59:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="8ce8a336-7ef7-4169-9311-24206b5c834c"
I0124 22:22:51.906962       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9969558599695586
I0124 22:22:51.911672       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9959411466260781
I0124 22:22:51.913103       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0
E0124 22:22:51.913855       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.59:8000/metrics: Get \"http://10.0.0.59:8000/metrics\": dial tcp 10.0.0.59:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="7fe2a51e-a1f1-49e1-9db2-b8dadf3c2aa9"
I0124 22:22:51.941195       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9974632166412989
I0124 22:22:51.944205       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9964485032978183
I0124 22:22:51.945624       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0
E0124 22:22:51.946384       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.59:8000/metrics: Get \"http://10.0.0.59:8000/metrics\": dial tcp 10.0.0.59:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="a300e490-fe45-4170-86d1-86d08f6f8433"
I0124 22:22:51.993752       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9979705733130391
I0124 22:22:51.996788       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0
E0124 22:22:51.997549       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.59:8000/metrics: Get \"http://10.0.0.59:8000/metrics\": dial tcp 10.0.0.59:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="c3d21a99-bedb-43d3-862c-8589d347336e"
I0124 22:22:52.085716       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9660071029934043
I0124 22:22:52.088924       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9984779299847792
I0124 22:22:52.091113       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0
E0124 22:22:52.091875       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.59:8000/metrics: Get \"http://10.0.0.59:8000/metrics\": dial tcp 10.0.0.59:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="8edbc042-9aaf-470b-aa99-17db2ad75322"
I0124 22:22:52.259426       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9756468797564688
I0124 22:22:52.266802       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9284627092846272
I0124 22:22:52.269308       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0
E0124 22:22:52.270118       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.59:8000/metrics: Get \"http://10.0.0.59:8000/metrics\": dial tcp 10.0.0.59:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="93328477-810c-46ef-8602-74b9ddccd17f"
I0124 22:22:52.598552       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9781836631151699
I0124 22:22:52.602700       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9299847792998478
I0124 22:22:52.605168       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0
E0124 22:22:52.605971       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.59:8000/metrics: Get \"http://10.0.0.59:8000/metrics\": dial tcp 10.0.0.59:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="3b3a2c3a-616e-473f-af56-9a1159ed53d1"
E0124 22:22:53.252275       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.59:8000/metrics: Get \"http://10.0.0.59:8000/metrics\": dial tcp 10.0.0.59:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="d551b1ae-a2a0-4283-9928-9c14f0f17312"
I0124 22:22:54.539968       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9477422628107559
I0124 22:22:54.541983       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0
E0124 22:22:54.542822       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.59:8000/metrics: Get \"http://10.0.0.59:8000/metrics\": dial tcp 10.0.0.59:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="e9ac2cd4-58fe-40d0-8cf0-ef0887976ca7"
I0124 22:22:57.110526       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9868087265347539
I0124 22:22:57.113538       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9715880263825469
I0124 22:22:57.115624       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0
E0124 22:22:57.116435       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.59:8000/metrics: Get \"http://10.0.0.59:8000/metrics\": dial tcp 10.0.0.59:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="24ae4af9-0b7b-49e7-8634-9991e9fcfcf6"
I0124 22:23:01.861821       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0
E0124 22:23:01.862664       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.59:8000/metrics: Get \"http://10.0.0.59:8000/metrics\": dial tcp 10.0.0.59:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="62109080-85e4-4d5f-b79e-8be64a76ef1a"
I0124 22:23:02.245826       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9147640791476408
I0124 22:23:02.247851       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0
E0124 22:23:02.248649       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.59:8000/metrics: Get \"http://10.0.0.59:8000/metrics\": dial tcp 10.0.0.59:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="2fd0f682-e6b5-4476-8d84-3e468d62127f"
I0124 22:23:11.865967       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9944190766108574
I0124 22:23:11.869773       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9944190766108574
I0124 22:23:11.872071       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0
E0124 22:23:11.873253       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.59:8000/metrics: Get \"http://10.0.0.59:8000/metrics\": dial tcp 10.0.0.59:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="2336270b-7860-43c0-bf1d-6a7e5b1b3417"
I0124 22:23:21.863902       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9791983764586505
I0124 22:23:21.867429       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9030948756976154
I0124 22:23:21.869396       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0
E0124 22:23:21.870204       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.59:8000/metrics: Get \"http://10.0.0.59:8000/metrics\": dial tcp 10.0.0.59:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="2d890a4f-02fe-4edc-aa1d-f2446b29bcb3"
I0124 22:23:22.736199       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9878234398782344
I0124 22:23:22.739219       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9081684424150178
I0124 22:23:22.740592       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0
E0124 22:23:22.741401       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.59:8000/metrics: Get \"http://10.0.0.59:8000/metrics\": dial tcp 10.0.0.59:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="bdd7776c-7cf9-4c5e-995a-5aef9ab98411"
I0124 22:23:31.863166       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9720953830542871
I0124 22:23:31.866744       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9482496194824962
I0124 22:23:31.869652       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0.421613394216134
E0124 22:23:31.872682       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to parse metrics from source http://10.0.0.59:8000/metrics: metrics gpu_cache_usage_perc not found" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="2aa4f046-ae32-484e-a0b6-08ec49ff5811"
I0124 22:23:41.863539       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9888381532217149
I0124 22:23:41.866986       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9396245560629122
I0124 22:23:41.869705       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0.7945205479452055
I0124 22:23:41.873049       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.59:8000/metrics" metricValue=0
I0124 22:23:41.873111       1 client.go:105] "Update pod list metrics" metricKey="default/aibrix-model-deepseek-llm-7b-chat" valueNum=4 timestamp="2025-01-24 22:23:41.859571106 +0000 UTC m=+350.231935734" metricValue=2.7229832572298327
I0124 22:23:41.873318       1 client.go:119] "Get panicWindow" metricKey="default/aibrix-model-deepseek-llm-7b-chat" panicValue=2.7229832572298327 panicWindow="TimeWindow(granularity=1s, window=window(size=1, values=[{1737757421, 2.72}]))"
I0124 22:23:41.873332       1 client.go:126] Get stableWindow: metricKey=default/aibrix-model-deepseek-llm-7b-chat, stableValue=2.72, stableWindow=TimeWindow(granularity=1s, window=window(size=1, values=[{1737757421, 2.72}]))
I0124 22:23:41.873356       1 kpa.go:330] "Operating in panic mode." desiredPodCount=6 desiredPanicPodCount=6
I0124 22:23:41.873375       1 kpa.go:336] "Increasing pods count." originalPodCount=3 desiredPodCount=6
I0124 22:23:41.873415       1 podautoscaler_controller.go:433] "Proposing desired replicas" desiredReplicas=6 metric="gpu_cache_usage_perc" timestamp="2025-01-24 22:23:41.873128879 +0000 UTC m=+350.245493509" scaleTarget="Deployment/default/aibrix-model-deepseek-llm-7b-chat"
I0124 22:23:41.888248       1 podautoscaler_controller.go:488] "Successfully rescaled" PodAutoscaler="default/podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" currentReplicas=4 desiredReplicas=6 reason="gpu_cache_usage_perc above target"
I0124 22:23:51.863439       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0.9812278031456114
I0124 22:23:51.868542       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.59:8000/metrics" metricValue=0
E0124 22:23:51.869343       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.105:8000/metrics: Get \"http://10.0.0.105:8000/metrics\": dial tcp 10.0.0.105:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="121408c0-0fd0-493f-aa41-220b7b285109"
I0124 22:23:51.880611       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.59:8000/metrics" metricValue=0
E0124 22:23:51.881717       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.105:8000/metrics: Get \"http://10.0.0.105:8000/metrics\": dial tcp 10.0.0.105:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="0e900448-1400-4bde-8d1f-bdb6bbcbb1ab"
I0124 22:23:51.898851       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0.9812278031456114
I0124 22:23:51.900666       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.59:8000/metrics" metricValue=0
E0124 22:23:51.901478       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.105:8000/metrics: Get \"http://10.0.0.105:8000/metrics\": dial tcp 10.0.0.105:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="24c7c932-f434-4de2-80cc-885dc3da72eb"
I0124 22:23:51.929166       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.59:8000/metrics" metricValue=0
E0124 22:23:51.930265       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.105:8000/metrics: Get \"http://10.0.0.105:8000/metrics\": dial tcp 10.0.0.105:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="35b3fdb9-5a28-41ed-9c89-55236d698e0d"
I0124 22:23:51.978673       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.59:8000/metrics" metricValue=0
E0124 22:23:51.979474       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.105:8000/metrics: Get \"http://10.0.0.105:8000/metrics\": dial tcp 10.0.0.105:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="ee615ad2-586e-40d3-a8fb-5f70c183faac"
I0124 22:23:52.067311       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9467275494672756
I0124 22:23:52.071077       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9893455098934552
I0124 22:23:52.073712       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0.9832572298325724
I0124 22:23:52.075771       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.59:8000/metrics" metricValue=0
E0124 22:23:52.076543       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.105:8000/metrics: Get \"http://10.0.0.105:8000/metrics\": dial tcp 10.0.0.105:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="965fb995-6690-4295-9dff-de0ba5a1c3fd"
I0124 22:23:52.246148       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.9487569761542365
I0124 22:23:52.249210       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9893455098934552
I0124 22:23:52.254764       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0.9512937595129376
I0124 22:23:52.256924       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.59:8000/metrics" metricValue=0
E0124 22:23:52.257723       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.105:8000/metrics: Get \"http://10.0.0.105:8000/metrics\": dial tcp 10.0.0.105:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="052e86a8-ab05-49fd-bcbb-4b67f4d03b3b"
E0124 22:23:52.583931       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.105:8000/metrics: Get \"http://10.0.0.105:8000/metrics\": dial tcp 10.0.0.105:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="e88851e7-cbe2-4763-a51c-6103b7220225"
E0124 22:23:53.230117       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.105:8000/metrics: Get \"http://10.0.0.105:8000/metrics\": dial tcp 10.0.0.105:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="f5760913-b8fe-46d2-8bd2-316eef09117f"
I0124 22:23:54.520898       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.943683409436834
I0124 22:23:54.523392       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0.9822425164890918
I0124 22:23:54.525826       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.59:8000/metrics" metricValue=0
E0124 22:23:54.526618       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.105:8000/metrics: Get \"http://10.0.0.105:8000/metrics\": dial tcp 10.0.0.105:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="ef157606-8bc3-4e6d-8763-2005263c1018"
I0124 22:23:57.094595       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.6930492135971589
I0124 22:23:57.097878       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.893455098934551
I0124 22:23:57.100564       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0.9797057331303907
I0124 22:23:57.102977       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.59:8000/metrics" metricValue=0
E0124 22:23:57.103764       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.105:8000/metrics: Get \"http://10.0.0.105:8000/metrics\": dial tcp 10.0.0.105:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="dceb67fe-18a5-43e1-a06e-0f9ad6c275ed"
I0124 22:24:01.866528       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.6260781329274481
I0124 22:24:01.869936       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9619482496194824
I0124 22:24:01.872434       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0.9680365296803652
I0124 22:24:01.874883       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.59:8000/metrics" metricValue=0
E0124 22:24:01.875675       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.105:8000/metrics: Get \"http://10.0.0.105:8000/metrics\": dial tcp 10.0.0.105:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="43c27b12-420f-4fe6-9445-f4d208fd6cef"
I0124 22:24:02.232779       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.6301369863013699
I0124 22:24:02.237798       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.964992389649924
I0124 22:24:02.240322       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0.9771689497716896
I0124 22:24:02.242620       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.59:8000/metrics" metricValue=0
E0124 22:24:02.243460       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.105:8000/metrics: Get \"http://10.0.0.105:8000/metrics\": dial tcp 10.0.0.105:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="56b911f1-7ff1-4f77-be05-fa159f834075"
I0124 22:24:11.862849       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.47691527143581935
I0124 22:24:11.868723       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9766615930999493
I0124 22:24:11.873726       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0.9036022323693557
I0124 22:24:11.877085       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.59:8000/metrics" metricValue=0
E0124 22:24:11.877950       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.105:8000/metrics: Get \"http://10.0.0.105:8000/metrics\": dial tcp 10.0.0.105:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="5fa91665-b26d-44d8-a8b6-f9410bf84e43"
I0124 22:24:21.862496       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.44393708777270424
I0124 22:24:21.866401       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9462201927955353
I0124 22:24:21.869416       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0.7853881278538812
I0124 22:24:21.871928       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.59:8000/metrics" metricValue=0
E0124 22:24:21.872709       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.105:8000/metrics: Get \"http://10.0.0.105:8000/metrics\": dial tcp 10.0.0.105:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="262c0e5e-901d-40c0-8de7-14df9777c40e"
I0124 22:24:22.731931       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9548452562151193
I0124 22:24:22.736173       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0.7985794013191274
I0124 22:24:22.738497       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.59:8000/metrics" metricValue=0
E0124 22:24:22.739422       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to fetch metrics from source http://10.0.0.105:8000/metrics: Get \"http://10.0.0.105:8000/metrics\": dial tcp 10.0.0.105:8000: connect: connection refused" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="2e657110-19c1-498f-91d0-fc4b20e1a30d"
E0124 22:24:31.868226       1 controller.go:329] "msg"="Reconciler error" "error"="failed to update metrics for scale target reference: failed to parse metrics from source http://10.0.0.105:8000/metrics: metrics gpu_cache_usage_perc not found" "PodAutoscaler"={"name":"podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa","namespace":"default"} "controller"="podautoscaler" "controllerGroup"="autoscaling.aibrix.ai" "controllerKind"="PodAutoscaler" "name"="podautoscaler-aibrix-model-deepseek-llm-7b-chat-kpa" "namespace"="default" "reconcileID"="e3a041ee-efaf-41cc-a432-0e2ec95a00bc"
I0124 22:24:41.864303       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0.43784880771182144
I0124 22:24:41.867966       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9786910197869102
I0124 22:24:41.871091       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0.9091831557584982
I0124 22:24:41.873628       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.59:8000/metrics" metricValue=0
I0124 22:24:41.877258       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.105:8000/metrics" metricValue=0
I0124 22:24:41.880255       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.58:8000/metrics" metricValue=0
I0124 22:24:41.880297       1 client.go:105] "Update pod list metrics" metricKey="default/aibrix-model-deepseek-llm-7b-chat" valueNum=6 timestamp="2025-01-24 22:24:41.860220093 +0000 UTC m=+410.232584721" metricValue=2.3257229832572297
I0124 22:24:41.880534       1 client.go:119] "Get panicWindow" metricKey="default/aibrix-model-deepseek-llm-7b-chat" panicValue=2.3257229832572297 panicWindow="TimeWindow(granularity=1s, window=window(size=1, values=[{1737757481, 2.33}]))"
I0124 22:24:41.880552       1 client.go:126] Get stableWindow: metricKey=default/aibrix-model-deepseek-llm-7b-chat, stableValue=2.33, stableWindow=TimeWindow(granularity=1s, window=window(size=1, values=[{1737757481, 2.33}]))
I0124 22:24:41.880574       1 kpa.go:320] "Exit panicking."
I0124 22:24:41.880588       1 kpa.go:343] "Operating in stable mode." desiredPodCount=5
I0124 22:24:41.880627       1 kpa.go:363] "Delaying scale to 5, staying at 6" desiredPodCount=5 delayedPodCount=6
I0124 22:24:41.880648       1 podautoscaler_controller.go:433] "Proposing desired replicas" desiredReplicas=6 metric="gpu_cache_usage_perc" timestamp="2025-01-24 22:24:41.880306217 +0000 UTC m=+410.252670847" scaleTarget="Deployment/default/aibrix-model-deepseek-llm-7b-chat"
I0124 22:24:51.864435       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0
I0124 22:24:51.868510       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9944190766108574
I0124 22:24:51.871238       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0.945712836123795
I0124 22:24:51.873649       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.59:8000/metrics" metricValue=0
I0124 22:24:51.876538       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.105:8000/metrics" metricValue=0
I0124 22:24:51.879871       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.58:8000/metrics" metricValue=0
I0124 22:24:51.879903       1 client.go:105] "Update pod list metrics" metricKey="default/aibrix-model-deepseek-llm-7b-chat" valueNum=6 timestamp="2025-01-24 22:24:51.860195982 +0000 UTC m=+420.232560611" metricValue=1.9401319127346524
I0124 22:24:51.880154       1 client.go:119] "Get panicWindow" metricKey="default/aibrix-model-deepseek-llm-7b-chat" panicValue=1.9401319127346524 panicWindow="TimeWindow(granularity=1s, window=window(size=1, values=[{1737757491, 1.94}]))"
I0124 22:24:51.880172       1 client.go:126] Get stableWindow: metricKey=default/aibrix-model-deepseek-llm-7b-chat, stableValue=2.13, stableWindow=TimeWindow(granularity=1s, window=window(size=2, values=[{1737757481, 2.33}, {1737757491, 1.94}]))
I0124 22:24:51.880193       1 kpa.go:343] "Operating in stable mode." desiredPodCount=5
I0124 22:24:51.880216       1 kpa.go:363] "Delaying scale to 5, staying at 6" desiredPodCount=5 delayedPodCount=6
I0124 22:24:51.880233       1 podautoscaler_controller.go:433] "Proposing desired replicas" desiredReplicas=6 metric="gpu_cache_usage_perc" timestamp="2025-01-24 22:24:51.879911028 +0000 UTC m=+420.252275653" scaleTarget="Deployment/default/aibrix-model-deepseek-llm-7b-chat"
I0124 22:25:01.863509       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0
I0124 22:25:01.867136       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.9903602232369355
I0124 22:25:01.869855       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0.9665144596651446
I0124 22:25:01.872317       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.59:8000/metrics" metricValue=0
I0124 22:25:01.875357       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.105:8000/metrics" metricValue=0
I0124 22:25:01.878544       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.58:8000/metrics" metricValue=0
I0124 22:25:01.878584       1 client.go:105] "Update pod list metrics" metricKey="default/aibrix-model-deepseek-llm-7b-chat" valueNum=6 timestamp="2025-01-24 22:25:01.859442337 +0000 UTC m=+430.231806981" metricValue=1.9568746829020802
I0124 22:25:01.878790       1 client.go:119] "Get panicWindow" metricKey="default/aibrix-model-deepseek-llm-7b-chat" panicValue=1.9568746829020802 panicWindow="TimeWindow(granularity=1s, window=window(size=1, values=[{1737757501, 1.96}]))"
I0124 22:25:01.878804       1 client.go:126] Get stableWindow: metricKey=default/aibrix-model-deepseek-llm-7b-chat, stableValue=2.07, stableWindow=TimeWindow(granularity=1s, window=window(size=3, values=[{1737757481, 2.33}, {1737757491, 1.94}, {1737757501, 1.96}]))
I0124 22:25:01.878824       1 kpa.go:343] "Operating in stable mode." desiredPodCount=5
I0124 22:25:01.878854       1 kpa.go:363] "Delaying scale to 5, staying at 6" desiredPodCount=5 delayedPodCount=6
I0124 22:25:01.878872       1 podautoscaler_controller.go:433] "Proposing desired replicas" desiredReplicas=6 metric="gpu_cache_usage_perc" timestamp="2025-01-24 22:25:01.878589277 +0000 UTC m=+430.250953899" scaleTarget="Deployment/default/aibrix-model-deepseek-llm-7b-chat"
I0124 22:25:11.862722       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.105:8000/metrics" metricValue=0
I0124 22:25:11.865822       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.58:8000/metrics" metricValue=0
I0124 22:25:11.869352       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0
I0124 22:25:11.872762       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.49873160832064944
I0124 22:25:11.877303       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0.974124809741248
I0124 22:25:11.880176       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.59:8000/metrics" metricValue=0
I0124 22:25:11.880214       1 client.go:105] "Update pod list metrics" metricKey="default/aibrix-model-deepseek-llm-7b-chat" valueNum=6 timestamp="2025-01-24 22:25:11.859421944 +0000 UTC m=+440.231786592" metricValue=1.4728564180618975
I0124 22:25:11.880429       1 client.go:119] "Get panicWindow" metricKey="default/aibrix-model-deepseek-llm-7b-chat" panicValue=1.4728564180618975 panicWindow="TimeWindow(granularity=1s, window=window(size=1, values=[{1737757511, 1.47}]))"
I0124 22:25:11.880446       1 client.go:126] Get stableWindow: metricKey=default/aibrix-model-deepseek-llm-7b-chat, stableValue=1.92, stableWindow=TimeWindow(granularity=1s, window=window(size=4, values=[{1737757481, 2.33}, {1737757491, 1.94}, {1737757501, 1.96}, {1737757511, 1.47}]))
I0124 22:25:11.880468       1 kpa.go:343] "Operating in stable mode." desiredPodCount=4
I0124 22:25:11.880514       1 kpa.go:363] "Delaying scale to 4, staying at 6" desiredPodCount=4 delayedPodCount=6
I0124 22:25:11.880567       1 podautoscaler_controller.go:433] "Proposing desired replicas" desiredReplicas=6 metric="gpu_cache_usage_perc" timestamp="2025-01-24 22:25:11.880220574 +0000 UTC m=+440.252585203" scaleTarget="Deployment/default/aibrix-model-deepseek-llm-7b-chat"
I0124 22:25:21.862576       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0
I0124 22:25:21.867604       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.49873160832064944
I0124 22:25:21.870478       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0.23947234906139014
I0124 22:25:21.873139       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.59:8000/metrics" metricValue=0
I0124 22:25:21.875644       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.105:8000/metrics" metricValue=0
I0124 22:25:21.878043       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.58:8000/metrics" metricValue=0
I0124 22:25:21.878106       1 client.go:105] "Update pod list metrics" metricKey="default/aibrix-model-deepseek-llm-7b-chat" valueNum=6 timestamp="2025-01-24 22:25:21.858542544 +0000 UTC m=+450.230907192" metricValue=0.7382039573820396
I0124 22:25:21.878379       1 client.go:119] "Get panicWindow" metricKey="default/aibrix-model-deepseek-llm-7b-chat" panicValue=0.7382039573820396 panicWindow="TimeWindow(granularity=1s, window=window(size=1, values=[{1737757521, 0.74}]))"
I0124 22:25:21.878397       1 client.go:126] Get stableWindow: metricKey=default/aibrix-model-deepseek-llm-7b-chat, stableValue=1.69, stableWindow=TimeWindow(granularity=1s, window=window(size=5, values=[{1737757481, 2.33}, {1737757491, 1.94}, {1737757501, 1.96}, {1737757511, 1.47}, {1737757521, 0.74}]))
I0124 22:25:21.878418       1 kpa.go:343] "Operating in stable mode." desiredPodCount=4
I0124 22:25:21.878449       1 kpa.go:363] "Delaying scale to 4, staying at 6" desiredPodCount=4 delayedPodCount=6
I0124 22:25:21.878464       1 podautoscaler_controller.go:433] "Proposing desired replicas" desiredReplicas=6 metric="gpu_cache_usage_perc" timestamp="2025-01-24 22:25:21.878121157 +0000 UTC m=+450.250485774" scaleTarget="Deployment/default/aibrix-model-deepseek-llm-7b-chat"
I0124 22:25:31.863281       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0
I0124 22:25:31.867209       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.4018264840182648
I0124 22:25:31.869947       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0.17858954845256214
I0124 22:25:31.872298       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.59:8000/metrics" metricValue=0
I0124 22:25:31.874516       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.105:8000/metrics" metricValue=0
I0124 22:25:31.877022       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.58:8000/metrics" metricValue=0
I0124 22:25:31.877083       1 client.go:105] "Update pod list metrics" metricKey="default/aibrix-model-deepseek-llm-7b-chat" valueNum=6 timestamp="2025-01-24 22:25:31.859455663 +0000 UTC m=+460.231820293" metricValue=0.580416032470827
I0124 22:25:31.877351       1 client.go:119] "Get panicWindow" metricKey="default/aibrix-model-deepseek-llm-7b-chat" panicValue=0.580416032470827 panicWindow="TimeWindow(granularity=1s, window=window(size=1, values=[{1737757531, 0.58}]))"
I0124 22:25:31.877370       1 client.go:126] Get stableWindow: metricKey=default/aibrix-model-deepseek-llm-7b-chat, stableValue=1.50, stableWindow=TimeWindow(granularity=1s, window=window(size=6, values=[{1737757481, 2.33}, {1737757491, 1.94}, {1737757501, 1.96}, {1737757511, 1.47}, {1737757521, 0.74}, {1737757531, 0.58}]))
I0124 22:25:31.877404       1 kpa.go:343] "Operating in stable mode." desiredPodCount=4
I0124 22:25:31.877451       1 kpa.go:363] "Delaying scale to 4, staying at 6" desiredPodCount=4 delayedPodCount=6
I0124 22:25:31.877472       1 podautoscaler_controller.go:433] "Proposing desired replicas" desiredReplicas=6 metric="gpu_cache_usage_perc" timestamp="2025-01-24 22:25:31.877098057 +0000 UTC m=+460.249462686" scaleTarget="Deployment/default/aibrix-model-deepseek-llm-7b-chat"
I0124 22:25:41.861882       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.58:8000/metrics" metricValue=0
I0124 22:25:41.865007       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0
I0124 22:25:41.869243       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.33079654997463215
I0124 22:25:41.872097       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0
I0124 22:25:41.874688       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.59:8000/metrics" metricValue=0
I0124 22:25:41.877325       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.105:8000/metrics" metricValue=0
I0124 22:25:41.877359       1 client.go:105] "Update pod list metrics" metricKey="default/aibrix-model-deepseek-llm-7b-chat" valueNum=6 timestamp="2025-01-24 22:25:41.859037201 +0000 UTC m=+470.231401847" metricValue=0.33079654997463215
I0124 22:25:41.877596       1 client.go:119] "Get panicWindow" metricKey="default/aibrix-model-deepseek-llm-7b-chat" panicValue=0.33079654997463215 panicWindow="TimeWindow(granularity=1s, window=window(size=1, values=[{1737757541, 0.33}]))"
I0124 22:25:41.877614       1 client.go:126] Get stableWindow: metricKey=default/aibrix-model-deepseek-llm-7b-chat, stableValue=1.17, stableWindow=TimeWindow(granularity=1s, window=window(size=6, values=[{1737757491, 1.94}, {1737757501, 1.96}, {1737757511, 1.47}, {1737757521, 0.74}, {1737757531, 0.58}, {1737757541, 0.33}]))
I0124 22:25:41.877642       1 kpa.go:343] "Operating in stable mode." desiredPodCount=3
I0124 22:25:41.877689       1 kpa.go:363] "Delaying scale to 3, staying at 6" desiredPodCount=3 delayedPodCount=6
I0124 22:25:41.877719       1 podautoscaler_controller.go:433] "Proposing desired replicas" desiredReplicas=6 metric="gpu_cache_usage_perc" timestamp="2025-01-24 22:25:41.877365253 +0000 UTC m=+470.249729875" scaleTarget="Deployment/default/aibrix-model-deepseek-llm-7b-chat"
I0124 22:25:51.861999       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.59:8000/metrics" metricValue=0
I0124 22:25:51.864609       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.105:8000/metrics" metricValue=0
I0124 22:25:51.867190       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.58:8000/metrics" metricValue=0
I0124 22:25:51.870328       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.26:8000/metrics" metricValue=0
I0124 22:25:51.874334       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.0.69:8000/metrics" metricValue=0.37341451040081175
I0124 22:25:51.877374       1 fetcher.go:122] "Successfully parsed metrics" metric="gpu_cache_usage_perc" source="http://10.0.1.28:8000/metrics" metricValue=0
I0124 22:25:51.877411       1 client.go:105] "Update pod list metrics" metricKey="default/aibrix-model-deepseek-llm-7b-chat" valueNum=6 timestamp="2025-01-24 22:25:51.859156157 +0000 UTC m=+480.231520816" metricValue=0.37341451040081175
I0124 22:25:51.877600       1 client.go:119] "Get panicWindow" metricKey="default/aibrix-model-deepseek-llm-7b-chat" panicValue=0.37341451040081175 panicWindow="TimeWindow(granularity=1s, window=window(size=1, values=[{1737757551, 0.37}]))"
I0124 22:25:51.877613       1 client.go:126] Get stableWindow: metricKey=default/aibrix-model-deepseek-llm-7b-chat, stableValue=0.91, stableWindow=TimeWindow(granularity=1s, window=window(size=6, values=[{1737757501, 1.96}, {1737757511, 1.47}, {1737757521, 0.74}, {1737757531, 0.58}, {1737757541, 0.33}, {1737757551, 0.37}]))
I0124 22:25:51.877636       1 kpa.go:343] "Operating in stable mode." desiredPodCount=3
I0124 22:25:51.877663       1 kpa.go:363] "Delaying scale to 3, staying at 6" desiredPodCount=3 delayedPodCount=6
I0124 22:25:51.877677       1 podautoscaler_controller.go:433] "Proposing desired replicas" desiredReplicas=6 metric="gpu_cache_usage_perc" timestamp="2025-01-24 22:25:51.877418006 +0000 UTC m=+480.249782630" scaleTarget="Deployment/default/aibrix-model-deepseek-llm-7b-chat"
